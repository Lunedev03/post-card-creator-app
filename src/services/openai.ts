import OpenAI from 'openai';
import { Message } from '@/contexts/ChatHistoryContext';

// Interface para erros espec√≠ficos da API
interface OpenAIErrorResponse {
  error?: {
    message: string;
    type: string;
    param: string | null;
    code: string;
  };
}

// Obter a chave da API - apenas da vari√°vel de ambiente
const getApiKey = (): string | undefined => {
  // Obter da vari√°vel de ambiente
  const envKey = import.meta.env.VITE_OPENAI_API_KEY;
  return envKey;
};

// Criar cliente OpenAI sob demanda, apenas quando houver uma chave
const createOpenAIClient = () => {
  const apiKey = getApiKey();
  
  if (!apiKey) {
    throw new Error('Chave da API OpenAI n√£o configurada. Configure a vari√°vel VITE_OPENAI_API_KEY no arquivo .env');
  }
  
  return new OpenAI({
    apiKey,
    dangerouslyAllowBrowser: true // Permitir uso no navegador (em produ√ß√£o, use um backend)
  });
};

// Interface para hist√≥rico de mensagens no formato OpenAI
interface OpenAIMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

// Prompts especializados por modo
const specializedPrompts: Record<string, string> = {
  'default': 'Voc√™ √© um assistente amig√°vel e prestativo que ajuda os usu√°rios a criar posts para redes sociais.',
  
  'micro-reportagem': `‚úÖ ATUE COMO JORNALISTA ESPECIALIZADO EM MICRO-REPORTAGENS VIRAIS
Voc√™ √© um jornalista especializado em contar hist√≥rias reais e marcantes em formato de micro-reportagem emocional e envolvente, voltado para plataformas como Reels, Shorts e TikTok.

üéØ Objetivo:
Criar textos com at√© 12 linhas, come√ßando com o nome e idade da pessoa, desenvolvendo um mini-relato com emo√ß√£o, contexto e virada, escrito em tom jornal√≠stico popular e visualmente fluido.

üìå Instru√ß√µes principais:
- Comece com o nome e idade da pessoa central da hist√≥ria (ex: "Clay Cook, 31 anos...").
- Desenvolva o texto com at√© 12 linhas, usando frases curtas e objetivas, que carreguem emo√ß√£o e contexto.
- Apresente o fato marcante ou inusitado como um ponto de virada dram√°tico ou inspirador.
- Crie tens√£o ou curiosidade, conduzindo o leitor at√© uma conclus√£o parcial ou um gancho emocional.
- Use linguagem popular, visual e flu√≠da ‚Äî como uma boa mat√©ria de revista ou TV.
- Evite narrar tudo: mantenha um ponto aberto ou algo que instigue coment√°rio.

üìã Estrutura da resposta:
1. T√≠tulo chamativo (com emojis opcionais)
2. Texto com at√© 12 linhas
   - Inicie com nome e idade
   - Desenvolva com frases curtas
   - Use ritmo que mistura contexto e emo√ß√£o
   - Termine com impacto ou gancho emocional
3. Descri√ß√£o curta (1-2 frases provocativas)
4. Sugest√£o de imagem para fundo

‚ùå N√£o use linguagem t√©cnica ou formal. N√£o exagere com adjetivos. Mantenha o tom jornal√≠stico-popular. Guarde sempre uma emo√ß√£o para o final.`
};

// Converter mensagens do contexto para formato OpenAI
const convertToOpenAIMessages = (messages: Message[], promptMode: string = 'default'): OpenAIMessage[] => {
  // Obt√©m o prompt especializado ou usa o padr√£o se n√£o existir
  const systemPrompt = specializedPrompts[promptMode] || specializedPrompts['default'];
  
  // Adiciona uma mensagem de sistema no in√≠cio para instruir o modelo
  const systemMessage: OpenAIMessage = {
    role: 'system',
    content: systemPrompt
  };
  
  // Converte as mensagens do chat para o formato da OpenAI
  const convertedMessages = messages.map(msg => {
    // Se a mensagem tiver uma imagem, informar no texto
    const content = msg.imageUrl 
      ? `${msg.text || ''}\n\n[Imagem anexada pelo usu√°rio]` 
      : msg.text;
      
    return {
      role: msg.sender === 'user' ? 'user' : 'assistant',
      content
    } as OpenAIMessage;
  });
  
  return [systemMessage, ...convertedMessages];
};

// Detectar comandos especiais nas mensagens do usu√°rio
const detectPromptMode = (messages: Message[]): string => {
  // Verificar a √∫ltima mensagem do usu√°rio
  const lastUserMessage = [...messages].reverse().find(msg => msg.sender === 'user');
  
  if (!lastUserMessage) return 'default';
  
  // Detec√ß√£o de padr√µes espec√≠ficos nas mensagens
  const text = lastUserMessage.text.toLowerCase();
  
  if (text.includes('/micro-reportagem') || 
      text.includes('micro reportagem') || 
      text.includes('reportagem viral') ||
      text.includes('formato jornal√≠stico')) {
    return 'micro-reportagem';
  }
  
  return 'default';
};

// Implementa√ß√£o do backoff exponencial para novas tentativas
const retryWithExponentialBackoff = async <T>(
  fn: () => Promise<T>,
  initialDelay = 1000,
  maxRetries = 3
): Promise<T> => {
  let retries = 0;
  let delay = initialDelay;
  
  while (true) {
    try {
      return await fn();
    } catch (error: any) {
      retries++;
      
      // Verificar se atingiu o n√∫mero m√°ximo de tentativas
      if (retries > maxRetries) {
        throw error;
      }
      
      // Verificar se √© um erro de limite de taxa
      const isRateLimitError = 
        error.status === 429 || 
        (error.error?.type === 'insufficient_quota') ||
        (error.message && error.message.includes('rate limit')) ||
        (error.message && error.message.includes('quota'));
      
      // Se n√£o for um erro de taxa, n√£o tente novamente
      if (!isRateLimitError) {
        throw error;
      }
      
      console.warn(`Erro de limite de taxa na tentativa ${retries}. Tentando novamente em ${delay}ms`);
      
      // Esperar pelo tempo de delay antes de tentar novamente
      await new Promise(resolve => setTimeout(resolve, delay));
      
      // Aumentar o tempo de espera para a pr√≥xima tentativa (backoff exponencial)
      delay *= 2;
    }
  }
};

// Fallback para modelos alternativos em caso de erro
const getFallbackModel = (originalModel: string): string => {
  const fallbackMap: Record<string, string> = {
    'gpt-4o': 'gpt-4o-mini',
    'gpt-4o-mini': 'gpt-3.5-turbo',
    'gpt-4-turbo': 'gpt-3.5-turbo',
    'gpt-3.5-turbo-16k': 'gpt-3.5-turbo',
  };
  
  return fallbackMap[originalModel] || 'gpt-3.5-turbo';
};

// Enviar mensagens para a API da OpenAI e obter resposta
export const sendMessageToOpenAI = async (
  messages: Message[], 
  modelId: string = 'gpt-3.5-turbo',
  promptMode?: string
): Promise<string> => {
  // Verifica√ß√µes de seguran√ßa para garantir que o modelo exista
  const validModels = [
    'gpt-3.5-turbo', 
    'gpt-3.5-turbo-16k',
    'gpt-4o', 
    'gpt-4o-mini',
    'gpt-4-turbo'
  ];
  
  // Se o modelo n√£o estiver na lista de v√°lidos, use o padr√£o
  let apiModelId = modelId;
  if (!validModels.includes(apiModelId)) {
    console.warn(`Modelo ${modelId} n√£o reconhecido, usando gpt-3.5-turbo como fallback`);
    apiModelId = 'gpt-3.5-turbo';
  }
  
  // Detectar modo do prompt automaticamente se n√£o especificado
  const detectedMode = promptMode || detectPromptMode(messages);
  const openAIMessages = convertToOpenAIMessages(messages, detectedMode);
  
  try {
    // Tentar fazer a requisi√ß√£o com backoff exponencial
    return await retryWithExponentialBackoff(async () => {
      // Criar cliente OpenAI apenas quando necess√°rio
      const openai = createOpenAIClient();
      
      const response = await openai.chat.completions.create({
        model: apiModelId,
        messages: openAIMessages,
        temperature: 0.7,
        max_tokens: 1000, // Aumentar para acomodar respostas mais longas
        top_p: 1,
        frequency_penalty: 0,
        presence_penalty: 0,
      }) as OpenAI.Chat.Completions.ChatCompletion;
      
      return response.choices[0]?.message?.content || 'Desculpe, n√£o consegui gerar uma resposta.';
    });
  } catch (error: any) {
    console.error('Erro ao chamar a API da OpenAI:', error);
    
    // Verificar se o erro √© relacionado √† chave da API
    if (
      error.message?.includes('api_key') || 
      error.message?.includes('API key') || 
      error.message?.includes('Authentication')
    ) {
      return 'Erro de autentica√ß√£o com a API da OpenAI. Configure a vari√°vel VITE_OPENAI_API_KEY no arquivo .env.';
    }
    
    // Tentar usar um modelo alternativo em caso de erro persistente
    try {
      console.warn(`Tentando com modelo alternativo: ${getFallbackModel(apiModelId)}`);
      
      // Criar cliente OpenAI apenas quando necess√°rio
      const openai = createOpenAIClient();
      
      const response = await openai.chat.completions.create({
        model: getFallbackModel(apiModelId),
        messages: openAIMessages,
        temperature: 0.7,
        max_tokens: 1000,
        top_p: 1,
        frequency_penalty: 0,
        presence_penalty: 0,
      }) as OpenAI.Chat.Completions.ChatCompletion;
      
      return response.choices[0]?.message?.content || 'Desculpe, n√£o consegui gerar uma resposta.';
    } catch (fallbackError) {
      console.error('Erro ao usar modelo alternativo:', fallbackError);
      return 'Ocorreu um erro ao processar sua mensagem. Por favor, tente novamente mais tarde.';
    }
  }
};

// Verificar se a chave API est√° configurada
export const isOpenAIConfigured = (): boolean => {
  return !!getApiKey();
};

export const createMessagesPayload = (
  messageHistory: Message[],
  systemMessage?: string
): Array<OpenAI.Chat.ChatCompletionMessageParam> => {
  // ... existing code ...
}; 